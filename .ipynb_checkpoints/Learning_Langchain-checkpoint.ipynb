{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b0a34f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91899cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d7ca086",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gauur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\llms\\openai.py:170: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "C:\\Users\\gauur\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\llms\\openai.py:624: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "llm  = OpenAI(model_name = \"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afef26c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Large language models are advanced artificial intelligence systems that are trained on huge amounts of text data to generate realistic and coherent human-like text.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"explain large language models in one sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cfbaf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca11d6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import(\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a481d92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Sure! Here is an example Python script that trains a simple neural network on simulated data using the TensorFlow library:\\n\\n```python\\nimport numpy as np\\nimport tensorflow as tf\\n\\n# Generate simulated data\\nnp.random.seed(0)\\nX = np.random.rand(100, 2)\\ny = np.random.randint(0, 2, 100)\\n\\n# Define the neural network architecture\\nmodel = tf.keras.models.Sequential([\\n    tf.keras.layers.Dense(64, activation='relu', input_shape=(2,)),\\n    tf.keras.layers.Dense(64, activation='relu'),\\n    tf.keras.layers.Dense(1, activation='sigmoid')\\n])\\n\\n# Compile the model\\nmodel.compile(optimizer='adam',\\n              loss='binary_crossentropy',\\n              metrics=['accuracy'])\\n\\n# Train the model\\nmodel.fit(X, y, epochs=10, batch_size=32)\\n\\n# Evaluate the model\\nloss, accuracy = model.evaluate(X, y)\\nprint(f'Loss: {loss}, Accuracy: {accuracy}')\\n```\\n\\nIn this script:\\n- We first generate some simulated data with 2 features and binary labels.\\n- We define a simple neural network with 2 hidden layers and compile it with binary cross-entropy loss.\\n- We train the model on the simulated data for 10 epochs.\\n- Finally, we evaluate the model on the same data and print the loss and accuracy.\\n\\nYou can run this script in a Python environment with TensorFlow installed to train a neural network on simulated data.\", additional_kwargs={})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\",temperature=0.3)\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are an expert data scientist\"),\n",
    "    HumanMessage(content=\"Write a Python script that trains a neural network on simulated data \")\n",
    "]\n",
    "response=chat(messages)\n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ad8f86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here is an example Python script that trains a simple neural network on simulated data using the TensorFlow library:\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "import tensorflow as tf\n",
      "\n",
      "# Generate simulated data\n",
      "np.random.seed(0)\n",
      "X = np.random.rand(100, 2)\n",
      "y = np.random.randint(0, 2, 100)\n",
      "\n",
      "# Define the neural network architecture\n",
      "model = tf.keras.models.Sequential([\n",
      "    tf.keras.layers.Dense(64, activation='relu', input_shape=(2,)),\n",
      "    tf.keras.layers.Dense(64, activation='relu'),\n",
      "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
      "])\n",
      "\n",
      "# Compile the model\n",
      "model.compile(optimizer='adam',\n",
      "              loss='binary_crossentropy',\n",
      "              metrics=['accuracy'])\n",
      "\n",
      "# Train the model\n",
      "model.fit(X, y, epochs=10, batch_size=32)\n",
      "\n",
      "# Evaluate the model\n",
      "loss, accuracy = model.evaluate(X, y)\n",
      "print(f'Loss: {loss}, Accuracy: {accuracy}')\n",
      "```\n",
      "\n",
      "In this script:\n",
      "- We first generate some simulated data with 2 features and binary labels.\n",
      "- We define a simple neural network with 2 hidden layers and compile it with binary cross-entropy loss.\n",
      "- We train the model on the simulated data for 10 epochs.\n",
      "- Finally, we evaluate the model on the same data and print the loss and accuracy.\n",
      "\n",
      "You can run this script in a Python environment with TensorFlow installed to train a neural network on simulated data.\n"
     ]
    }
   ],
   "source": [
    "print(response.content,end='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64d014f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "733438f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "template = \"\"\"\n",
    "You are an expert data scientist with an expertise in building deep learning models. \n",
    "Explain the concept of {concept} in a couple of lines\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"concept\"],\n",
    "    template=template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8df58a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['concept'], output_parser=None, partial_variables={}, template='\\nYou are an expert data scientist with an expertise in building deep learning models. \\nExplain the concept of {concept} in a couple of lines\\n', template_format='f-string', validate_template=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0b5c02c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'An autoencoder is a type of artificial neural network used for unsupervised learning that aims to learn efficient representations of input data by encoding it into a latent space and then decoding it back to the original input. It is often used for tasks such as dimensionality reduction, data denoising, and anomaly detection.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(prompt.format(concept=\"autoencoder\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c2b725c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "190af336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An autoencoder is a type of neural network that learns to encode data in a lower-dimensional representation and then decode it back to its original form. It is commonly used for tasks such as dimensionality reduction, data compression, and anomaly detection.\n"
     ]
    }
   ],
   "source": [
    "# Import LLMChain and define chain with language model and prompt as arguments.\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Run the chain only specifying the input variable.\n",
    "print(chain.run(\"autoencoder\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7867dd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a second prompt \n",
    "\n",
    "second_prompt = PromptTemplate(\n",
    "    input_variables=[\"ml_concept\"],\n",
    "    template=\"Turn the concept description of {ml_concept} and explain it to me like I'm five in 500 words\",\n",
    ")\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6644fc3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mAn autoencoder is a type of neural network that learns to encode input data into a more compact representation and then decode it back to its original form. It is commonly used for tasks such as data compression, denoising, and feature extraction in machine learning and deep learning applications.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mAlright, imagine you have a magic puzzle box that can turn big, messy pictures into smaller, simpler ones. First, you put a picture in the box, and the box learns how to squish it down into a tiny puzzle. Then, when you want the big picture back, the box can un-squish the puzzle and make the big picture appear again!\n",
      "\n",
      "This magic puzzle box is called an autoencoder. It's like a special brain that can take a complicated picture, like one with lots of details and colors, and turn it into a smaller, simplified version. It's kind of like when you take a big jigsaw puzzle and fit all the pieces together to make a complete picture.\n",
      "\n",
      "Autoencoders are really smart because they can do lots of cool things with pictures and other types of data. One of the things they can do is help make files smaller so they don't take up as much space on your computer. This is like cleaning up your room and putting all your toys away neatly so you have more room to play.\n",
      "\n",
      "They can also help get rid of any mistakes or fuzzy parts in a picture, like when you're drawing and accidentally make a squiggle in the wrong place. The autoencoder can fix those little mistakes and make the picture look nice and clear again.\n",
      "\n",
      "Another fun thing autoencoders can do is find the most important parts of a picture or a piece of information. It's like picking out the best toys to play with from a big box of toys. This helps make the picture or information really easy to understand and helps us learn new things.\n",
      "\n",
      "Autoencoders are like little wizards that can transform things into simpler, more organized versions. They're really helpful when we want to save space, fix mistakes, or focus on the most important parts of something.\n",
      "\n",
      "So, remember, an autoencoder is like a magic puzzle box that can turn big, messy pictures into small, neat ones, and then turn them back again whenever you want. It's a cool tool that helps us do all sorts of fun and useful things with our pictures and information!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Alright, imagine you have a magic puzzle box that can turn big, messy pictures into smaller, simpler ones. First, you put a picture in the box, and the box learns how to squish it down into a tiny puzzle. Then, when you want the big picture back, the box can un-squish the puzzle and make the big picture appear again!\n",
      "\n",
      "This magic puzzle box is called an autoencoder. It's like a special brain that can take a complicated picture, like one with lots of details and colors, and turn it into a smaller, simplified version. It's kind of like when you take a big jigsaw puzzle and fit all the pieces together to make a complete picture.\n",
      "\n",
      "Autoencoders are really smart because they can do lots of cool things with pictures and other types of data. One of the things they can do is help make files smaller so they don't take up as much space on your computer. This is like cleaning up your room and putting all your toys away neatly so you have more room to play.\n",
      "\n",
      "They can also help get rid of any mistakes or fuzzy parts in a picture, like when you're drawing and accidentally make a squiggle in the wrong place. The autoencoder can fix those little mistakes and make the picture look nice and clear again.\n",
      "\n",
      "Another fun thing autoencoders can do is find the most important parts of a picture or a piece of information. It's like picking out the best toys to play with from a big box of toys. This helps make the picture or information really easy to understand and helps us learn new things.\n",
      "\n",
      "Autoencoders are like little wizards that can transform things into simpler, more organized versions. They're really helpful when we want to save space, fix mistakes, or focus on the most important parts of something.\n",
      "\n",
      "So, remember, an autoencoder is like a magic puzzle box that can turn big, messy pictures into small, neat ones, and then turn them back again whenever you want. It's a cool tool that helps us do all sorts of fun and useful things with our pictures and information!\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "overall_chain = SimpleSequentialChain(chains=[chain, chain_two], verbose=True)\n",
    "\n",
    "# Run the chain specifying only the input variable for the first chain.\n",
    "explanation = overall_chain.run(\"autoencoder\")\n",
    "print(explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6a4bbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings and vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c1b2b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import utility for splitting up texts and split up the explanation given above into document chunks\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 100,\n",
    "    chunk_overlap  = 0,\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents([explanation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc7adfee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alright, imagine you have a magic puzzle box that can turn big, messy pictures into smaller, simpler'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a5fe0d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "be85937c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Encoding 'cl100k_base'>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose an appropriate encoding name\n",
    "encoding_name = \"cl100k_base\"  # Common encoding for many models\n",
    "\n",
    "# Get the tokenizer for the specified encoding\n",
    "encoding = tiktoken.get_encoding(encoding_name)\n",
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c98b1afe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[72586,\n",
       " 11,\n",
       " 13085,\n",
       " 499,\n",
       " 617,\n",
       " 264,\n",
       " 11204,\n",
       " 25649,\n",
       " 3830,\n",
       " 430,\n",
       " 649,\n",
       " 2543,\n",
       " 2466,\n",
       " 11,\n",
       " 46946,\n",
       " 9364,\n",
       " 1139,\n",
       " 9333,\n",
       " 11,\n",
       " 35388]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = encoding.encode(texts[0].page_content)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a865e718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alright, imagine you have a magic puzzle box that can turn big, messy pictures into smaller, simpler'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_chunk = encoding.decode(tokens)\n",
    "text_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "79d338fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model_name=\"ada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ee5207a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ededd32d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([72586,    11, 13085,   499,   617,   264, 11204, 25649,  3830,\n",
       "         430,   649,  2543,  2466,    11, 46946,  9364,  1139,  9333,\n",
       "          11, 35388])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_vector = np.array(tokens)\n",
    "tokens_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "13f54fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: [72586, 11, 13085, 499, 617, 264, 11204, 25649, 3830, 430, 649, 2543, 2466, 11, 46946, 9364, 1139, 9333, 11, 35388]\n",
      "Tokens Vector: [72586    11 13085   499   617   264 11204 25649  3830   430   649  2543\n",
      "  2466    11 46946  9364  1139  9333    11 35388]\n",
      "Loaded Vector: [72586    11 13085   499   617   264 11204 25649  3830   430   649  2543\n",
      "  2466    11 46946  9364  1139  9333    11 35388]\n"
     ]
    }
   ],
   "source": [
    "# Print the tokens and the vector\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Tokens Vector:\", tokens_vector)\n",
    "\n",
    "# Save the vector to a file if needed\n",
    "np.save(\"tokens_vector.npy\", tokens_vector)\n",
    "\n",
    "# To load the vector back\n",
    "loaded_vector = np.load(\"tokens_vector.npy\")\n",
    "print(\"Loaded Vector:\", loaded_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "227d6005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and initialize Pinecone client\n",
    "\n",
    "import os\n",
    "import pinecone\n",
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "\n",
    "pinecone.init(\n",
    "    api_key=os.getenv('PINECONE_API_KEY'),  \n",
    "    environment=os.getenv('PINECONE_ENV')  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9e4bf547",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No active indexes found in your Pinecone project, are you sure you're using the right API key and environment?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[69], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Upload vectors to Pinecone\u001b[39;00m\n\u001b[0;32m      3\u001b[0m index_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlangchain-quickstart\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m search \u001b[38;5;241m=\u001b[39m \u001b[43mPinecone\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\vectorstores\\base.py:164\u001b[0m, in \u001b[0;36mVectorStore.from_documents\u001b[1;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[0;32m    162\u001b[0m texts \u001b[38;5;241m=\u001b[39m [d\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m    163\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [d\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m--> 164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\vectorstores\\pinecone.py:214\u001b[0m, in \u001b[0;36mPinecone.from_texts\u001b[1;34m(cls, texts, embedding, metadatas, ids, batch_size, text_key, index_name, namespace, **kwargs)\u001b[0m\n\u001b[0;32m    212\u001b[0m     index \u001b[38;5;241m=\u001b[39m pinecone\u001b[38;5;241m.\u001b[39mIndex(index_name)\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(indexes) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 214\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    215\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo active indexes found in your Pinecone project, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    216\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mare you sure you\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre using the right API key and environment?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    217\u001b[0m     )\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndex \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not found in your Pinecone project. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDid you mean one of the following indexes: \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.join(indexes)}\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    222\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: No active indexes found in your Pinecone project, are you sure you're using the right API key and environment?"
     ]
    }
   ],
   "source": [
    "# Upload vectors to Pinecone\n",
    "\n",
    "index_name = \"langchain-quickstart\"\n",
    "search = Pinecone.from_documents(texts, embeddings, index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c4d66338",
   "metadata": {},
   "outputs": [],
   "source": [
    "#agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6149b326",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.agent_toolkits import create_python_agent\n",
    "from langchain.tools.python.tool import PythonREPLTool\n",
    "from langchain.python import PythonREPL\n",
    "from langchain.llms.openai import OpenAI\n",
    "\n",
    "agent_executor = create_python_agent(\n",
    "    llm=OpenAI(model_name = \"gpt-3.5-turbo\",temperature=0, max_tokens=1000),\n",
    "    tool=PythonREPLTool(),\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bcb41a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mWe can use the quadratic formula to find the roots of the quadratic function.\n",
      "Action: Python REPL\n",
      "Action Input: import math\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mWe need to define the coefficients a, b, and c of the quadratic function.\n",
      "Action: Python REPL\n",
      "Action Input: a = 3, b = 2, c = -1\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3minvalid syntax. Maybe you meant '==' or ':=' instead of '='? (<string>, line 1)\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI made a mistake in defining the coefficients. Let me correct that.\n",
      "Action: Python REPL\n",
      "Action Input: a = 3\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI need to define the other coefficients as well.\n",
      "Action: Python REPL\n",
      "Action Input: b = 2\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI need to define the last coefficient.\n",
      "Action: Python REPL\n",
      "Action Input: c = -1\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mNow that I have defined the coefficients, I can use the quadratic formula to find the roots.\n",
      "Action: Python REPL\n",
      "Action Input: root1 = (-b + math.sqrt(b**2 - 4*a*c)) / (2*a)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI need to calculate the second root using the quadratic formula.\n",
      "Action: Python REPL\n",
      "Action Input: root2 = (-b - math.sqrt(b**2 - 4*a*c)) / (2*a)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: The roots of the quadratic function 3 * x**2 + 2*x -1 are x = 0.333 and x = -1.0\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The roots of the quadratic function 3 * x**2 + 2*x -1 are x = 0.333 and x = -1.0'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.run(\"Find the roots (zeros) if the quadratic function 3 * x**2 + 2*x -1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569ae2a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
